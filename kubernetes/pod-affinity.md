# ğŸ”Œ Pod Affinity

![](../.gitbook/assets/img2.jpg)

BazÄ± durumlarda, bizler oluÅŸturduÄŸumuz podun schedule edileceÄŸi node Ã¼stÃ¼nde Ã§alÄ±ÅŸan baÅŸka podlarÄ±n olmasÄ±nÄ± veya olmamasÄ±nÄ± isteyebiliriz.&#x20;

Misal, Azure Ã¼stÃ¼nde koÅŸan kubernetes clusterÄ±mÄ±z var. Bu kubernete clusterÄ±n 4 worker node var. Ve bu 4 worker node da, aynÄ± region iÃ§erisinde bulunan 2 farklÄ± availability zone Ã¼zerinde daÄŸÄ±tÄ±lmÄ±ÅŸtÄ±r.&#x20;

Yani worker node 1 ve worker node 2 availability zone 1 de, worker node 3 ve worker node 4 de availability zone 2 de barÄ±nmaktadÄ±r.

Farzedelim ki biz frontent , cache ve veritabanÄ±ndan oluÅŸan bir uygulama deploy edeceÄŸiz. Ä°lk olarak veritabanÄ±mÄ±zÄ± oluÅŸturacak pod tanÄ±mÄ±nÄ± hazÄ±rladÄ±k ve ve kubernetes api gÃ¶nderdik ve kubernetes scheduler iÅŸini yaptÄ± ve podu Ã§alÄ±ÅŸtÄ±racaÄŸÄ± uygun bir node buldu. Ve varsayalÄ±m ki bu node ise, worker node 1, sonrasÄ±nda frontend uygulamasÄ±nÄ± deploy ettik ve onu da worker node 3 de schedule etti. Son olarak da, cache podumuzu deploy ettik ve bu pod da son olarak worker node 4 de Ã§alÄ±ÅŸmaya baÅŸladÄ±.

Bu senaryo da bir kaÃ§ sÄ±kÄ±ntÄ± var,

Ä°lk olarak, frontend podumuz ve veritabanÄ± podumuz ayrÄ± ayrÄ± availability zonelar Ã¼zerinde schedule edildiler. Tahmin edebileceÄŸiniz Ã¼zere frontend podumuz sÃ¼rekli olarak veritabanÄ± ile gÃ¶rÃ¼ÅŸÃ¼yor. SÃ¼rekli ona veri yazÄ±yor ve ondan veri okuyor. Hemen hemen tÃ¼m cloud servis saÄŸlayÄ±cÄ±lar 2 availability zone arasÄ±nda veri transferine Ã¼crete tabi tutulur. Yani biz 2 podu da aynÄ± availability zone Ã¼zerinde bulunan worker nodelarda koÅŸtursaydÄ±k, aynÄ± availability zone Ã¼zerinde durduklarÄ± iÃ§in aralarÄ±ndaki veri transferine para Ã¶demeyecekken, ÅŸimdi ayrÄ± availability zone da durduklarÄ± iÃ§in para Ã¶demek zorunda kalacaÄŸÄ±z. Bunun olmasÄ±nÄ± engellemek iÃ§in, node affinity yazarak, podlarÄ± aynÄ± yere toplayabiliriz. Ama bu seferde her ÅŸeyi manuel olarak ayarlamak gerekecek. Fakat bunun yerine ÅŸÃ¶yle bir ÅŸey yapabilseydik, sanÄ±rÄ±m iÅŸleri otomatik olarak kubernetes hallederdi. EÄŸer ben pod tanÄ±mÄ±na, bak bu podu oluÅŸturacaÄŸÄ±n node'u seÃ§erken git veritabanÄ± poduna bak ve o pod hangi  availability zone da oluÅŸturulduysa, bu podu da orada oluÅŸtur gibi bir tanÄ±m ekleyebilseydim bu sorunu Ã§Ã¶zebilirdik.

DiÄŸer bir istediÄŸimiz bir durumda ÅŸu olabilirdi;

Misal frontend ve cache podlarÄ± ayrÄ± worker node Ã¼zerinde oluÅŸturulmuÅŸ. BunlarÄ± da keÅŸke aynÄ± worker node Ã¼zerinde Ã§alÄ±ÅŸtÄ±rabilseydim. SonuÃ§ta cache ve frontend arasÄ±nda ne kadar az latency olursa, uygulamam o kadar performanslÄ± Ã§alÄ±ÅŸÄ±r.

Bunun kolay yolu da ÅŸu olurdu, cache pod tanÄ±mÄ±nda ÅŸunu diyebilseydik, git frontend pod'a bak, o hangi node Ã¼stÃ¼nde oluÅŸturulduysa, bu pod da orada oluÅŸturulsun.&#x20;

Bize tÃ¼m bunlarÄ± saÄŸlayan, Kubernetes Ã¶zelliÄŸi mevcut.

**Pod Affinity podumuzun hangi worker node Ã¼zerinde oluÅŸturulmaya uygun olduÄŸunu nodelardaki etiketlere gÃ¶re deÄŸil, hali hazÄ±rda node da Ã§alÄ±ÅŸmakta olan podlardaki etiketlere gÃ¶re sÄ±nÄ±rlamamÄ±za olanak tanÄ±r.**

Bizlerin bir pod tanÄ±mÄ±na ekleyerek, o podun oluÅŸturulacaÄŸÄ± node seÃ§imini ve o node Ã¼zerinde Ã§alÄ±ÅŸan veya Ã§alÄ±ÅŸmayan podlarÄ±n varlÄ±ÄŸÄ±na gÃ¶re yapabilme imkanÄ± veren Ã¶zelliÄŸe Pod Affinity diyoruz. NasÄ±l ki, node affinity de bir podun oluÅŸturulacaÄŸÄ± node ve o node da bulunan veya bulunmayan labellara gÃ¶re seÃ§ebiliyorsak, Pod Affinity de ise, bunu ilgili node Ã¼zerinde Ã§alÄ±ÅŸan pod olup, olmadÄ±ÄŸÄ±na gÃ¶re seÃ§ebiliyoruz.&#x20;



Bizler daha Ã¶nceden worker nodelarÄ±mÄ±za Ã§eÅŸitli labellar eklemeyi gÃ¶rdÃ¼k ve ekledik. Fakat nodelar Ã¼zerinde Ã¶nceden de tanÄ±mlanmÄ±ÅŸ(default gelen) bir Ã§ok label mevcuttur.

Her kubernetes node da,

<mark style="color:orange;">1 -</mark> <mark style="color:red;">kubernetes.io/arch</mark> <mark style="color:orange;">=> Ä°lgili node x64 veya ARM tabanlÄ± bir iÅŸlemciye sahip olduÄŸunu belirtir. DeÄŸer olarak Amd64 veya ARM deÄŸerlerini alabilir.</mark>\
\ <mark style="color:orange;">2 -</mark> <mark style="color:red;">kubernetes.io/hostname</mark> <mark style="color:orange;">=> Ä°lgili node'un, hostname deÄŸerini alÄ±r.</mark>

<mark style="color:orange;">3 -</mark> <mark style="color:red;">kubernetes.io/os</mark> <mark style="color:orange;">=> Ä°lgili node'un iÅŸletim sistemi bilgilerini alÄ±r (Linux,Windows)</mark>

LabellarÄ± bulunur.

<mark style="color:purple;">Bunun yanÄ±nda, cloud servis saÄŸlayÄ±cÄ±larÄ±nÄ±n yÃ¶netilen Kubernetes hizmetindeki nodelarda ilgili node'un hangi region da bulunduÄŸunu belirten (</mark> <mark style="color:yellow;">topology.kubernetes.io/region</mark> <mark style="color:purple;">) ve hangi availability zone da bulunduÄŸu belirten, (</mark> <mark style="color:yellow;">topology.kubernetes.io/zone</mark> <mark style="color:purple;">) anahtarlarÄ± ile belirtilir.</mark>&#x20;



Ã–rnek bir Pod Affinity tanÄ±mÄ± iÃ§eren YAML dosyasÄ± iÃ§eriÄŸi,

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: frontendpod
  labels:
    app: frontend
    deployment: test
spec:
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
---
apiVersion: v1
kind: Pod
metadata:
  name: cachepod
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - frontend
        topologyKey: kubernetes.io/hostname
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: color
              operator: In
              values:
              - blue
          topologyKey: kubernetes.io/hostname
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: deployment
              operator: In
              values:
              - prod
          topologyKey: topology.kubernetes.io/zone
  containers:
  - name: cachecontainer
    image: redis:6-alpine
```

2\. Pod tanÄ±nÄ± inceleyelim, Pod affinity tanÄ±mlarÄ±da aynen node affinity tanÄ±mlarÄ± gibi requiredDuringScheduling ve preferredDuringScheduling olarak hard ve soft gereksinimlere sahip olabilirler. Node affinity tanÄ±mlara oldukÃ§a benzer. Tek bir fark vardÄ±r. Pod affinity tanÄ±mlarÄ±onda topologyKey adÄ±nda bir daha gireriz. Ä°lk tanÄ±mda requiredDuringSecheduling seÃ§eneÄŸi girili, yani mecbur kÄ±lÄ±nmÄ±ÅŸ. Bu tanÄ±mda diyoruz ki, schedule edileceÄŸi worker node Ã¼zerinde app=frontend labelina sahip bir pod olmalÄ±. Yani bu podu bu ÅŸartÄ± saÄŸlayan host Ã¼stÃ¼nde schedule et. topologyKey kÄ±smÄ±nda bunu belirtiyoruz. Burada hostname anahatarÄ±nÄ± seÃ§ersek, aynÄ± hostname yani aynÄ± worker node Ã¼zerinde bu pod Ã§alÄ±ÅŸtÄ±rÄ±lacak.&#x20;

Misal, diyelim ki hostname yerine "zone" anahtarÄ±nÄ± seÃ§tik. Bu seferde aynÄ± worker node olmasÄ±na gerek yok olmayacaktÄ±. Buraya topology.kubernetes.io/zone yazsaydÄ±k, bu podu app=frontend labelli bir podun Ã§alÄ±ÅŸtÄ±ÄŸÄ± availability zone daki herhangi bir worker node Ã¼zerinde Ã§alÄ±ÅŸtÄ±r. ManasÄ±na gelecekti.



GÃ¶rÃ¼ldÃ¼ÄŸÃ¼ gibi node affinity de Ã§alÄ±ÅŸacaÄŸÄ± worker nodedaki labellara gÃ¶re seÃ§iyorduk. Pod affinitty de ise, hangi podlarla birlikte Ã§alÄ±ÅŸacaÄŸÄ±nÄ± seÃ§iyoruz.

Pod Affinity'nin Node Affinity gÃ¶re bir farkÄ± da anti-affinity tanÄ±mlarÄ±nÄ±n ayrÄ± yapÄ±lmasÄ±dÄ±r. node anti affinity kÄ±smÄ±nÄ± not-in ile halledebiliyorduk. Fakat pod anti affinity kÄ±smÄ±nÄ± Ã¶zel olarak tanÄ±mlamamÄ±z gerekiyor. Bunu da podAntiAffinity anahtarÄ± ile yapÄ±yoruz.

Ã–rnekte bulunan antiAffinity kÄ±smÄ±nÄ±  inceleyecek olursak,\
Bu podu deployment=prod tanÄ±mÄ±na sahip olan bir pod ile, aynÄ± zone da schedule etme diyoruz.

Bu YAML dosyasÄ±nÄ± deploy edersek ÅŸu olur;

* Ã–ncelikle frontend pod adÄ±ndaki pod schedule edilecek. GÃ¶rdÃ¼ÄŸÃ¼nÃ¼z gibi app=frontend labeli eklemiÅŸiz. Bu da bizim frontend podumuzu temsil ediyor.
* SonrasÄ±nda ise, 2.podumuz olan, cachepod isimli podumuz deploy ederken, kubernetes pod affinity kurallarÄ±na bakacak, Ä°lk kural olan RequireDuring kÄ±smÄ±ndan bahsedersek, burada ÅŸunu diyoruz, Bu pod mutlaka app=frontend labelina sahip bir pod ile aynÄ± node Ã¼zerinde Ã§alÄ±ÅŸtÄ±rÄ±lsÄ±n. EÄŸer bu ÅŸekilde birden fazla node varsa, git color=blue labelina sahip bir podun Ã§alÄ±ÅŸtÄ±ÄŸÄ± worker node varsa, onu seÃ§. Ve mÃ¼mkÃ¼nse bu seÃ§tiÄŸin worker node buludunduÄŸu zone iÃ§erisinde deployment=prod labelina sahip bir pod bulunmasÄ±n.
