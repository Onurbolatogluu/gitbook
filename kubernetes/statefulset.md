# ğŸ—¿ StatefulSet

![](../.gitbook/assets/StatefulSets.webp)

Kubernetes'in stateless iÅŸ yÃ¼kleri iÃ§in, uygun bir platform olduÄŸundan, state tutan iÅŸ yÃ¼kleri iÃ§in ise bir Ã§ok problem ile karÅŸÄ±laÅŸtÄ±ÄŸÄ±mÄ±zdan Ã¶nceki bÃ¶lÃ¼mlerde sÃ¶z etmiÅŸtik.&#x20;

Ephemeral ve Persistent volume konularÄ±nda ise, bu sorunlarÄ± ortadan kaldÄ±rmak adÄ±na, kullanabileceÄŸimiz objeleri gÃ¶rdÃ¼k ve iÅŸin depolama kÄ±smÄ±nÄ± Ã§Ã¶zdÃ¼k. Ancak stateful uygulamalarda, tek sÄ±kÄ±ntÄ±mÄ±z depolama deÄŸil. Bunu Ã§Ã¶zmÃ¼ÅŸ olsak da, ÅŸu ana kadar iÅŸlediÄŸimiz obje tipleri diÄŸer sorunlarÄ±mÄ±zÄ± Ã§Ã¶zmÃ¼yor.&#x20;

Ä°ÅŸte bu durum ile baÅŸa Ã§Ä±kmamÄ±za yardÄ±mcÄ± olacak obje tipi olan, statefulSet objesinden bahsedeceÄŸiz.

Sorun nedir?

Ä°lk olarak kendimize sorun oluÅŸturalÄ±m; 3 pod oluÅŸturacak, bir deployment objesi oluÅŸturduÄŸumuzu dÃ¼ÅŸÃ¼nelim. Bu deployment objesi, bildiÄŸiniz Ã¼zere bir replicaset oluÅŸturuyor ve bu replicaset 'de, bizim belirlediÄŸimiz tanÄ±ma gÃ¶re random isimler verilen, birbirinin aynÄ±sÄ± podlar oluÅŸturuyor.

Bu replicaset altÄ±nda oluÅŸturulan podlarÄ±n, birbirinden farkÄ± yok. Ve bu nedenle kubernetes bunlara eÅŸit davranÄ±yor. Biz bu deployment objesini, yukarÄ± doÄŸru scale ettiÄŸimiz zaman, istediÄŸimiz sayÄ±da yeni pod ekleniyor ve aÅŸaÄŸÄ± doÄŸru scale edersek de, bu sefer bu podlardan istediÄŸimiz sayÄ±da pod **rastgele** olarak siliniyor.&#x20;

Yani 5 podlu deployment objesini, 3 pod'a indirmek istersek, kubernetes ilk hangisi yaratÄ±lmÄ±ÅŸ, hangisi diÄŸerinden Ã¶nce yaratÄ±lmÄ±ÅŸ vb ÅŸeylere bakmÄ±yor ve rastgele olarak 2 podu siliyor. Podlar Ã¼zerinde herhangi bir state tutulmadÄ±ÄŸÄ± iÃ§in bu durum sÄ±kÄ±ntÄ± yaratmÄ±yor. &#x20;

Fakat ÅŸÃ¶yle bir senaryo da,

Misal, yeni nesil noSQL veritabanlarÄ±ndan birini kubernetes Ã¼zerinde deploy etmek istiyoruz. Bu veritabanlarÄ± hemen, hemen hepsi ÅŸu ÅŸekilde Ã§alÄ±ÅŸÄ±r. Bir adet master instance bulunur ve bunun Ã¼zerinden bir cluster oluÅŸturulur. ArdÄ±ndan bu cluster'a yeni instance'lar ekleriz. Yazma iÅŸlemlerini master Ã¼zerinden yaparken, sorgularÄ± herhangi bir instance'a gÃ¶nderebilirsiniz.

TÃ¼m veri bu istancelar Ã¼stÃ¼nde daÄŸÄ±tÄ±k ÅŸekilde durur.&#x20;

BÃ¶yle bir veritabanÄ± alanÄ± olan, apache cassandra 'yÄ± 3 instance'dan oluÅŸacak, bir cluster olacak ÅŸekilde ÅŸu ana kadar Ã¶ÄŸrendiÄŸimiz obje tipleri ile kubernetes'e deploy ettiÄŸimizi dÃ¼ÅŸÃ¼nÃ¼n,&#x20;

Bunun iÃ§in nasÄ±l bir yol izleyeceÄŸiz?

Cassandra master olacak instance'Ä± deploy edeceÄŸiz, ardÄ±ndan buna baÄŸlanarak yada baÅŸlangÄ±Ã§ komutlarÄ± ile cassandra uygulamasÄ±nÄ± master hale getirecek ÅŸekilde, bir cluster oluÅŸturacaÄŸÄ±z. SonrasÄ±nda 2. instance oluÅŸturacaÄŸÄ±z ve bu 2. instance baÄŸlanÄ±p, 1.instance'daki cassandra cluster'a  2.instance'Ä± dahil edeceÄŸiz. ArdÄ±ndan 3.instance oluÅŸturup, aynÄ± iÅŸlemleri yapacaÄŸÄ±z. BÃ¶ylelikle 3. instance ile bir cassandra cluster'mÄ±z olacak.

Kubernetes'de nasÄ±l ayaÄŸa kaldÄ±racaÄŸÄ±z?

Bu adÄ±mlarÄ± takip ederek, Kubernetes de nasÄ±l bu uygulamayÄ± ayaÄŸa kaldÄ±racaÄŸÄ±mÄ±za bakalÄ±m,

Ä°lk olarak singleton podlar ÅŸeklinde deploy etmeyi deneyelim, Master olacak instance oluÅŸturacak pod tanÄ±mÄ±nÄ± yaptÄ±k iÃ§ine PVC tanÄ±mÄ± ekledik ki, cassandra verileri tutabilecek, bir  PV'ye sahip olabilsin. Pod ayaÄŸa kalktÄ±, bunu master hale getirecek komutlarÄ±, ya baÅŸlangÄ±Ã§ komutu olarak yada sonrasÄ±nda baÄŸlanÄ±p hallettik. ArdÄ±ndan 2. pod tanÄ±mÄ± daha yaptÄ±k, bunu da, 2.instance olarak ayaÄŸa kaldÄ±rdÄ±k. ArdÄ±dan 3.pod tanÄ±mÄ±nÄ± yapÄ±p, ayaÄŸa kaldÄ±rdÄ±k. Ve bÃ¶ylelikle gerekli ayarlarÄ± yaptÄ±ktan sonra, cluster ayaÄŸa kalktÄ±.&#x20;

Ancak burada bir Ã§ok sorun var,&#x20;

\-Ä°ÅŸlemler manuel ve zahmetli\
\-Singleton podlarÄ±n fail durumunu kontrol eden bir mekanizma yok.\
\-Yeni bir pod eklemek veya Ã§Ä±karmak istersem, tÃ¼m sÃ¼reci yÃ¶netiyorum.

SingletonPod iÅŸimize yaramadÄ±, Peki deployment kullansak?

3 adet cassandra podu oluÅŸturacak, deployment objesi oluÅŸturduk ve deploy ettik. Bu deployment objesi 3 adet birbirinin aynÄ±sÄ± pod oluÅŸturdu ve hepsine rastgele bir isim verdi.  Problem ÅŸu ki, hepsi aynÄ± anda oluÅŸturuldu. Ben ilk oluÅŸanÄ± master olarak ayarlamak istesem de, ayarlayamam. Ã‡Ã¼nkÃ¼ hepsi, bu sorun dersek, bu iÅŸlemleri manuel olarak yapalÄ±m. BaÄŸlanÄ±p, birini master'a Ã§evirelim, DiÄŸerlerine de tek, tek baÄŸlanÄ±p cluster'a kattÄ±k. Aradan zaman geÃ§ti, worker nodelardan birinde sÄ±kÄ±ntÄ± oldu ve cassandra podlarÄ±ndan birine eriÅŸilemiyor.&#x20;

Deployment objemiz tarafÄ±ndan yÃ¶netildiÄŸi iÃ§in, sÄ±kÄ±ntÄ± deÄŸil, hemen yeni bir pod oluÅŸturulur. Ama bu yeni pod tamamen rastgele bir isim alacak ve sÄ±fÄ±rdan oluÅŸacak. Misal bu bahsettiÄŸimiz pod Master instance olarak ayarladÄ±ÄŸÄ±mÄ±z Pod ise ne yapacaÄŸÄ±z? Cluster bu durumda Ã§Ã¶kmÃ¼ÅŸ olacak.

Yada ÅŸunu dÃ¼ÅŸÃ¼nÃ¼n, scale down veya up iÅŸlemi yaptÄ±ÄŸÄ±mÄ±zda, kubernetes buraya rastgele bir pod ekleyecek veya rastgele olarak iÃ§lerinden birini silecek. BunlarÄ±n hepsi stateless uygulamalar iÃ§in problem deÄŸil, fakat gÃ¶rdÃ¼ÄŸÃ¼nÃ¼z Ã¼zere cluster uygulamalarÄ± vb stateful uygulamalardan oluÅŸan clusterlar kurmak istersek, mevcut kubernetes objeleri sÄ±kÄ±ntÄ±mÄ±zÄ± Ã§Ã¶zmÃ¼yor.

![](../.gitbook/assets/1\_QrSDTMp-x8AZJYdQNdWacw.jpeg)

TÃ¼m bu sÄ±kÄ±ntÄ±larÄ± Ã§Ã¶zmek adÄ±na, statefulset adÄ±nda bir obje mevcuttur.

* Statefulset tarafÄ±ndan oluÅŸturulan her pod, statefulset tanÄ±mÄ±nda belirlediÄŸimiz PVC'ye gÃ¶re bir PV'ye sahip olur. Yani her podun kendine ait bir PV'si olur.
* Statefulset altÄ±nda, podlar sÄ±rayla oluÅŸturulup, sÄ±rayla silinir. Misal 3 podlu bir stateful oluÅŸturduÄŸumuz zaman, Ã¶ncelikle pod0 oluÅŸturulur. Ve bu pod ayaÄŸa kalkÄ±p, readiness ve liveness checklerinden geÃ§ip, iÅŸlemlerini tamamlamadan bir sonraki pod oluÅŸturulmaz. Ne zaman pod hazÄ±r ve running duruma geÃ§er, o zaman bir sonraki pod oluÅŸturulur.  AynÄ± ÅŸekilde 3.podda 2.pod tamamlanmadan oluÅŸturulmaz. Tam tersi durumda da bu geÃ§erlidir. Biz 3 podlu bir statefulseti 2 poda indirirseniz, kubernetes random bir pod seÃ§ip, onu silme yoluna gitmez.  En son yaratÄ±lan pod hangisi ise, ilk olarak o silinir.&#x20;
* Statefulset tarafÄ±ndan oluÅŸturulan her pod'a statefulsetismi-0 1 2 3 ÅŸeklinde devam eden, sabit bir isim verilir. Ä°simler random seÃ§ilmez ve bu isimler aynÄ± zamanda containerlarÄ±n hostname olarak da ayarlandÄ±ÄŸÄ± iÃ§in, her uygulama bu isimle ulaÅŸabilir.&#x20;

Statefulset objesi, deployment objesine oldukÃ§a benzerdir. YukarÄ±da bahsettiÄŸim Ã¶zellikler haricinde aynÄ±dÄ±r. Bu Ã¶zellikler sayesinde, bizlerin stateful uygulamalarÄ± deploy etmemiz oldukÃ§a kolaylaÅŸÄ±r.

Az Ã¶nceki cassandra Ã¶rneÄŸini statefulset ile ayaÄŸa kaldÄ±racak olursak,  3 pod oluÅŸturacak bir statefulset objesi yaratÄ±p, deploy ettik. Kubernetes hemen ilk podu ayaÄŸa kaldÄ±rdÄ±. Biz bu poda baÅŸlangÄ±Ã§ scripti eklemiÅŸtik. Bu script ortamda bizim belirlediÄŸimiz isimde cluster var mÄ±? yok mu? bunu kontrol ediyor. Ve yoksa bu isimle bir cluster oluÅŸturuyor. Script Ã§alÄ±ÅŸtÄ±, ortamda bu isimle cluster olmadÄ±ÄŸÄ± iÃ§in, cassandra cluster'Ä± oluÅŸturuldu. Liveness ve readiness probe tamamlandÄ± ve artÄ±k hem cluster, hem de podumuz hazÄ±r.

Birinci pod saÄŸlÄ±klÄ± ÅŸekilde Ã§alÄ±ÅŸmaya baÅŸladÄ±ÄŸÄ± anda Kubernetes 2.podu da oluÅŸturmaya baÅŸladÄ±, Bu podda da, aynÄ± script var, ama bu sefer ortamda bÃ¶yle bir cluster mevcut, dolayÄ±sÄ±yla yeni bir cluster oluÅŸturmak yerine, bu mevcut cluster'a dahil oldu. Liveness ve readiness probe tamamlandÄ± ve bu pod hazÄ±r.&#x20;

Bu pod'da hazÄ±r olduÄŸu iÃ§in 3.pod da oluÅŸturulmaya baÅŸlandÄ±, onda da aynÄ± iÅŸlemler uygulandÄ± ve cluster ayaÄŸa kalktÄ±.&#x20;

Åimdi bu podlardan, cassandra-1 podu silinirse, statefulset objesi, aynÄ± podu, aynÄ± isimle tekrar ayaÄŸa kaldÄ±rÄ±p, aynÄ± PV bu poda atanacak. ve sorun olmayacak.&#x20;

Yada yeni pod eklersek, yeni identy olacak ve aynÄ± ÅŸekilde cluster'a katÄ±lacak. Scale down olursa da, herhangi birini seÃ§ip, silmek yerine en son oluÅŸturulan silinecek. GÃ¶rdÃ¼ÄŸÃ¼nÃ¼z gibi, cassandra cluster oluÅŸturma problemini bu Ã¶zellikleri sayesinde "statefulset" ile Ã§Ã¶zebildik.&#x20;

Ã–rnek statefulset objesi yaml iÃ§eriÄŸi,

```yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: cassandra
  name: cassandra
spec:
  clusterIP: None
  ports:
  - port: 9042
  selector:
    app: cassandra
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cassandra
  labels:
    app: cassandra
spec:
  serviceName: cassandra
  replicas: 3
  selector:
    matchLabels:
      app: cassandra
  template:
    metadata:
      labels:
        app: cassandra
    spec:
      terminationGracePeriodSeconds: 1800
      containers:
      - name: cassandra
        image: gcr.io/google-samples/cassandra:v13
        imagePullPolicy: Always
        ports:
        - containerPort: 7000
          name: intra-node
        - containerPort: 7001
          name: tls-intra-node
        - containerPort: 7199
          name: jmx
        - containerPort: 9042
          name: cql
        resources:
          limits:
            cpu: "500m"
            memory: 1Gi
          requests:
            cpu: "500m"
            memory: 1Gi
        securityContext:
          capabilities:
            add:
              - IPC_LOCK
        lifecycle:
          preStop:
            exec:
              command: 
              - /bin/sh
              - -c
              - nodetool drain
        env:
          - name: MAX_HEAP_SIZE
            value: 512M
          - name: HEAP_NEWSIZE
            value: 100M
          - name: CASSANDRA_SEEDS
            value: "cassandra-0.cassandra.default.svc.cluster.local"
          - name: CASSANDRA_CLUSTER_NAME
            value: "K8Demo"
          - name: CASSANDRA_DC
            value: "DC1-K8Demo"
          - name: CASSANDRA_RACK
            value: "Rack1-K8Demo"
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - /ready-probe.sh
          initialDelaySeconds: 15
          timeoutSeconds: 5
        volumeMounts:
        - name: cassandra-data
          mountPath: /cassandra_data
  volumeClaimTemplates:
  - metadata:
      name: cassandra-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: standard
      resources:
        requests:
          storage: 1Gi
```

Replicas sayÄ±mÄ±zÄ± belirtiyor ve label selector'a dikkat ediyoruz. ArdÄ±ndan template kÄ±smÄ±nda ise, oluÅŸturulmasÄ±nÄ± istediÄŸimiz pod tanÄ±mÄ±nÄ± giriyoruz. template iÃ§eriÄŸi kullandÄ±ÄŸÄ±mÄ±z image'a gÃ¶re deÄŸiÅŸir.

Template kÄ±smÄ± haricinde, en altta "VolumeClaimTemplates" kÄ±smÄ± mevcut. Burada her bir pod iÃ§in, bu Ã¶zelliklerde bir PVC oluÅŸturulmasÄ±nÄ± sÃ¶ylÃ¼yoruz.  Bu PVClerde standart isimli storage class'Ä± kullanarak, her bir pod iÃ§in birer PV oluÅŸturacak.&#x20;

Yani her podun, aynÄ± PV'ye baÄŸlanmasÄ± yerine, her pod'a ayrÄ± bir PV oluÅŸturulacak. PVCleri VolumeClaimTemplates ile yaratÄ±yoruz. PVCleri dahi kendimiz manuel olarak oluÅŸturmuyoruz. statefulset objesini oluÅŸturduÄŸumuz zaman, PVC oluÅŸturulacak, PVClerde gidecek PVleri oluÅŸturup, podlara tek, tek baÄŸlayacak.

En Ã¼st kÄ±sÄ±mda "service" tanÄ±mÄ± mevcut, gÃ¶rdÃ¼ÄŸÃ¼nÃ¼z gibi ÅŸu ana kadar oluÅŸturduÄŸumuz service tanÄ±mÄ±na oldukÃ§a benzer. Fakat tek fark mevcut, clusterIP=none olarak set edilmiÅŸ. Buna headless services diyoruz. Bunu oluÅŸturduÄŸumuz anda, ClusterIP tipinde, bir servis oluÅŸturulacak, Fakat ana bir IP adresi atanmayacak. Bu bize ÅŸu konuda yardÄ±mcÄ± oluyor; Ben ne zaman, bu servis ismine gitmek istersem, o bana bu servis altÄ±ndaki podlardan bir tanesinin IP adresini dÃ¶necek. Bunun yanÄ±nda her bir podda da, podismi.serviceismi ÅŸeklinde eriÅŸme imkanÄ± saÄŸlamÄ±ÅŸ olacak.

![](../.gitbook/assets/1951\_XIhmbB\_qGimfr.png)
